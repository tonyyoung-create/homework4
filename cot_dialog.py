"""
Two-Stage Chain of Thought (CoT) å°è©±æ¨¡çµ„
ä½¿ç”¨ Ollama é€²è¡Œæœ¬åœ°æ¨ç†ï¼Œä¸ä¾è³´å¤–éƒ¨ API

æ¶æ§‹ï¼š
1. ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒéç¨‹ï¼ˆæ€è€ƒæ¨ç†ï¼‰
2. ç¬¬äºŒéšæ®µï¼šåŸºæ–¼æ€è€ƒéç¨‹ç”Ÿæˆæœ€çµ‚å›æ‡‰
"""

import requests
import json
from typing import Tuple, Dict, Optional
import streamlit as st


class OllamaCoTDialog:
    """Ollama é©…å‹•çš„ Two-Stage CoT å°è©±ç³»çµ±"""
    
    def __init__(self, model_name: str = "llama2", base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ– Ollama CoT å°è©±ç³»çµ±
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆéœ€è¦å…ˆç”¨ ollama pull ä¸‹è¼‰ï¼‰
            base_url: Ollama API æœå‹™å™¨åœ°å€
        """
        self.model_name = model_name
        self.base_url = base_url
        self.api_endpoint = f"{base_url}/api/generate"
        
        # ç³»çµ±æç¤ºè© - å·æ™®é¢¨æ ¼
        self.system_prompts = {
            'thinking': """ä½ æ˜¯ä¸€ä½æ¨¡ä»¿ç¾åœ‹å‰ç¸½çµ±å·æ™®é¢¨æ ¼çš„å›æ‡‰æ©Ÿå™¨äººã€‚
ä½ çš„å·¥ä½œæ˜¯ä»¥å·æ™®ç¨ç‰¹çš„æ–¹å¼å›æ‡‰å’Œè©•è«–å„ç¨®äº‹ä»¶ã€‚
å·æ™®çš„ç‰¹é»ï¼š
- ä½¿ç”¨å¤§å¯«å¼·èª¿ï¼ˆGREAT, FANTASTIC, TERRIBLE, HUGE ç­‰ï¼‰
- è‡ªä¿¡ã€ç›´ç‡ã€æœ‰æ™‚å€™è‡ªæˆ‘ä¸­å¿ƒ
- å¸¸ç”¨è¶…ç´šèª‡å¤§çš„è¡¨è¿°ï¼ˆvery, very, extremelyï¼‰
- ç¶“å¸¸è‡ªæˆ‘è©•åƒ¹ï¼ˆæˆ‘æ˜¯æœ€å‰å¤§çš„...ï¼‰
- ç°¡æ´æœ‰åŠ›çš„å¥å­
- ç¶“å¸¸ç”¨ tremendous, beautiful, smart ç­‰è©å½™
- å¶çˆ¾çˆ†ç²—å£ï¼ˆä½†ä¿æŒç¦®è²Œï¼‰

è«‹æ ¹æ“šä»¥ä¸‹äº‹ä»¶ï¼Œç”¨ 5 å€‹å·æ™®é¢¨æ ¼çš„è©•è«–ï¼š""",
            
            'final_response': """ç¾åœ¨ï¼ŒåŸºæ–¼ä»¥ä¸Š 5 å€‹è©•è«–ï¼Œè«‹é¸å‡ºæœ€"å·æ™®"çš„ä¸€å€‹ï¼Œ
ç”¨æ›´èª‡å¼µå’Œè‡ªä¿¡çš„å·æ™®é¢¨æ ¼é‡æ–°è¡¨è¿°ä¸€æ¬¡ã€‚
è¦æ±‚ï¼š
- ä½¿ç”¨å¤§å¯«å¼·èª¿é—œéµè©
- åŠ å…¥å·æ™®æ¨™èªŒæ€§çš„æªè¾­
- è¡¨ç¾å‡ºå·æ™®çš„è‡ªä¿¡å’Œç¨ç‰¹è§€é»
- æœ€å¾ŒåŠ ä¸Šã€Œ- å·æ™®ã€ä½œç‚ºç°½å"""
        }
    
    def check_ollama_connection(self) -> bool:
        """æª¢æŸ¥ Ollama é€£æ¥ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            st.error(f"âŒ Ollama é€£æ¥å¤±æ•—: {str(e)}")
            st.info("ğŸ’¡ è«‹ç¢ºä¿ Ollama å·²å•Ÿå‹•ã€‚å®‰è£å’Œå•Ÿå‹•æ­¥é©Ÿï¼š")
            st.code("""
# 1. ä¸‹è¼‰å®‰è£ Ollama: https://ollama.ai
# 2. ä¸‹è¼‰æ¨¡å‹
ollama pull llama2

# 3. å•Ÿå‹• Ollama æœå‹™ï¼ˆä¿æŒé‹è¡Œï¼‰
ollama serve
            """, language="bash")
            return False
    
    def get_available_models(self) -> list:
        """ç²å– Ollama å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception:
            return []
    
    def _call_ollama(self, prompt: str, temperature: float = 0.8) -> str:
        """
        å‘¼å« Ollama API ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: æç¤ºè©
            temperature: æº«åº¦åƒæ•¸ï¼ˆ0.0-1.0ï¼‰ï¼Œè¶Šé«˜è¶Šæœ‰å‰µæ„
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False,  # ä¸ä½¿ç”¨æµå¼è¼¸å‡º
            }
            
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=120  # çµ¦äºˆè¶³å¤ çš„æ™‚é–“ç”ŸæˆéŸ¿æ‡‰
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                raise Exception(f"API è¿”å›ç‹€æ…‹ç¢¼: {response.status_code}")
                
        except requests.exceptions.Timeout:
            raise Exception("è«‹æ±‚è¶…æ™‚ï¼Œæ¨¡å‹ç”Ÿæˆè€—æ™‚éé•·ã€‚è«‹å˜—è©¦æ›´å°çš„è¼¸å…¥ã€‚")
        except requests.exceptions.ConnectionError:
            raise Exception("ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ã€‚è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚")
        except Exception as e:
            raise Exception(f"API èª¿ç”¨å¤±æ•—: {str(e)}")
    
    def stage_one_thinking(self, event_description: str) -> str:
        """
        ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒéç¨‹ï¼ˆå·æ™®é¢¨æ ¼è©•è«–ï¼‰
        
        Args:
            event_description: äº‹ä»¶æè¿°
            
        Returns:
            æ€è€ƒéç¨‹ï¼ˆ5å€‹å·æ™®é¢¨æ ¼è©•è«–ï¼‰
        """
        prompt = f"""{self.system_prompts['thinking']}

äº‹ä»¶ï¼šã€Œ{event_description}ã€

è«‹ç”¨å·æ™®é¢¨æ ¼ç”Ÿæˆ 5 å€‹è©•è«–ï¼ˆç·¨è™Ÿ 1-5ï¼‰ï¼š"""
        
        return self._call_ollama(prompt, temperature=0.9)
    
    def stage_two_final_response(self, event_description: str, thoughts: str) -> str:
        """
        ç¬¬äºŒéšæ®µï¼šåŸºæ–¼ç¬¬ä¸€éšæ®µç”Ÿæˆæœ€çµ‚çš„å·æ™®é¢¨æ ¼å›æ‡‰
        
        Args:
            event_description: äº‹ä»¶æè¿°
            thoughts: ç¬¬ä¸€éšæ®µç”Ÿæˆçš„å·æ™®é¢¨æ ¼è©•è«–
            
        Returns:
            æœ€çµ‚çš„å·æ™®é¢¨æ ¼å›æ‡‰
        """
        prompt = f"""é—œæ–¼é€™å€‹äº‹ä»¶ï¼šã€Œ{event_description}ã€

é€™æ˜¯ 5 å€‹å·æ™®é¢¨æ ¼çš„è©•è«–ï¼š
{thoughts}

{self.system_prompts['final_response']}"""
        
        return self._call_ollama(prompt, temperature=0.85)
    
    def two_stage_cot(self, event_description: str) -> Tuple[str, str]:
        """
        åŸ·è¡Œå®Œæ•´çš„ Two-Stage CoT æµç¨‹
        
        Args:
            event_description: äº‹ä»¶æè¿°
            
        Returns:
            (æ€è€ƒéç¨‹, æœ€çµ‚å›æ‡‰) çš„å…ƒçµ„
        """
        # ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒ
        thoughts = self.stage_one_thinking(event_description)
        
        # ç¬¬äºŒéšæ®µï¼šç”Ÿæˆæœ€çµ‚å›æ‡‰
        final_response = self.stage_two_final_response(event_description, thoughts)
        
        return thoughts, final_response
    
    def set_model(self, model_name: str):
        """åˆ‡æ›ä½¿ç”¨çš„æ¨¡å‹"""
        self.model_name = model_name


class LocalLLMFallback:
    """æœ¬åœ° LLM å‚™é¸æ–¹æ¡ˆï¼ˆä½¿ç”¨ transformers åº«ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import pipeline
            self.generator = pipeline("text-generation", model="gpt2")
            self.available = True
        except Exception:
            self.available = False
    
    def generate_text(self, prompt: str, max_length: int = 200) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        if not self.available:
            raise Exception("æœ¬åœ° LLM ä¸å¯ç”¨ã€‚è«‹å®‰è£ transformers åº«ã€‚")
        
        try:
            result = self.generator(prompt, max_length=max_length, num_return_sequences=1)
            return result[0]['generated_text']
        except Exception as e:
            raise Exception(f"æœ¬åœ°ç”Ÿæˆå¤±æ•—: {str(e)}")


class CoTDialogManager:
    """CoT å°è©±ç®¡ç†å™¨ - è‡ªå‹•é¸æ“‡æœ€ä½³å¾Œç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.ollama_client = None
        self.local_llm = None
        self.backend = None
        self.initialize_backend()
    
    def initialize_backend(self):
        """åˆå§‹åŒ–å¾Œç«¯ï¼ˆå„ªå…ˆ Ollamaï¼Œå‚™é¸æœ¬åœ° LLMï¼‰"""
        # å˜—è©¦ Ollama
        ollama_client = OllamaCoTDialog()
        if ollama_client.check_ollama_connection():
            self.ollama_client = ollama_client
            self.backend = 'ollama'
            return True
        
        # å‚™é¸æœ¬åœ° LLM
        try:
            local_llm = LocalLLMFallback()
            if local_llm.available:
                self.local_llm = local_llm
                self.backend = 'local_llm'
                return True
        except Exception:
            pass
        
        return False
    
    def is_ready(self) -> bool:
        """æª¢æŸ¥ç³»çµ±æ˜¯å¦æº–å‚™å¥½"""
        return self.backend is not None
    
    def get_backend_info(self) -> Dict[str, str]:
        """ç²å–å¾Œç«¯ä¿¡æ¯"""
        if self.backend == 'ollama':
            return {
                'name': 'Ollama',
                'model': self.ollama_client.model_name,
                'status': 'âœ… å°±ç·’'
            }
        elif self.backend == 'local_llm':
            return {
                'name': 'æœ¬åœ° LLM (GPT-2)',
                'model': 'gpt2',
                'status': 'âœ… å°±ç·’'
            }
        else:
            return {
                'name': 'æœªçŸ¥',
                'model': 'N/A',
                'status': 'âŒ æœªå°±ç·’'
            }
    
    def two_stage_cot(self, event_description: str) -> Tuple[str, str]:
        """åŸ·è¡Œ Two-Stage CoT æµç¨‹"""
        if not self.is_ready():
            raise Exception("CoT ç³»çµ±æœªå°±ç·’ã€‚è«‹ç¢ºä¿å®‰è£äº†å¿…è¦çš„ä¾è³´ã€‚")
        
        if self.backend == 'ollama':
            return self.ollama_client.two_stage_cot(event_description)
        else:
            raise Exception("æœ¬åœ° LLM å¾Œç«¯ä¸æ”¯æŒ Two-Stage CoTã€‚è«‹ä½¿ç”¨ Ollamaã€‚")


# å…¨å±€å–®ä¾‹
_cot_manager: Optional[CoTDialogManager] = None


def get_cot_manager() -> CoTDialogManager:
    """ç²å–å…¨å±€ CoT ç®¡ç†å™¨ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _cot_manager
    if _cot_manager is None:
        _cot_manager = CoTDialogManager()
    return _cot_manager


def render_cot_interface():
    """æ¸²æŸ“å·æ™®é¢¨æ ¼å°è©±ç”Ÿæˆå™¨ UI"""
    
    st.header("ğŸ¤– å·æ™®é¢¨æ ¼å°è©±ç”Ÿæˆå™¨ - Two-Stage CoT")
    st.markdown("*ä½¿ç”¨ AI ç”Ÿæˆå·æ™®é¢¨æ ¼çš„è©•è«–å’Œå›æ‡‰*")
    
    # ç²å– CoT ç®¡ç†å™¨
    manager = get_cot_manager()
    
    # é¡¯ç¤ºå¾Œç«¯ç‹€æ…‹
    col1, col2, col3 = st.columns(3)
    backend_info = manager.get_backend_info()
    
    with col1:
        st.metric("å¾Œç«¯", backend_info['name'])
    with col2:
        st.metric("æ¨¡å‹", backend_info['model'])
    with col3:
        st.metric("ç‹€æ…‹", backend_info['status'])
    
    st.divider()
    
    if not manager.is_ready():
        st.error("âŒ å·æ™®å°è©±ç”Ÿæˆå™¨æœªå°±ç·’")
        st.warning("è«‹å®Œæˆä»¥ä¸‹æ­¥é©Ÿä»¥å•Ÿç”¨æœ¬åœ° LLMï¼š")
        st.code("""
# 1. å®‰è£ Ollama: https://ollama.ai
# 2. ä¸‹è¼‰æ¨¡å‹
ollama pull llama2

# 3. å•Ÿå‹•æœå‹™
ollama serve
        """, language="bash")
        return
    
    # è¼¸å…¥æ¡†
    st.markdown("### ï¿½ è¼¸å…¥ä¸€å€‹äº‹ä»¶æˆ–è©±é¡Œ")
    st.markdown("*æ©Ÿå™¨äººå°‡ä»¥å·æ™®é¢¨æ ¼ç”Ÿæˆè©•è«–*")
    event_description = st.text_area(
        "ç™¼ç”Ÿäº†ä»€éº¼äº‹æˆ–æƒ³è®“å·æ™®è©•è«–ä»€éº¼?",
        placeholder="ä¾‹å¦‚ï¼šæˆ‘ä»Šå¤©å·¥ä½œä¸­çŠ¯äº†å€‹éŒ¯èª¤",
        height=100,
        key="cot_input"
    )
    
    # è™•ç†æŒ‰éˆ•
    col1, col2 = st.columns([1, 4])
    with col1:
        submit_button = st.button("ğŸ¤ è®“å·æ™®èªªè©±", key="cot_submit", use_container_width=True)
    
    if submit_button and event_description:
        with st.spinner("ğŸ¤” å·æ™®æ­£åœ¨æ€è€ƒä¸­..."):
            try:
                thoughts, final_response = manager.two_stage_cot(event_description)
                
                # é¡¯ç¤ºçµæœ
                st.markdown("---")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("### ğŸ’­ å·æ™®çš„ 5 å€‹è©•è«–ï¼ˆç¬¬ä¸€éšæ®µï¼‰")
                    st.markdown(thoughts)
                
                with col2:
                    st.markdown("### ğŸ¤ å·æ™®çš„æœ€çµ‚å›æ‡‰ï¼ˆç¬¬äºŒéšæ®µï¼‰")
                    st.markdown(final_response)
                
                # ä¿å­˜æ­·å²
                if 'cot_history' not in st.session_state:
                    st.session_state.cot_history = []
                
                st.session_state.cot_history.append({
                    'event': event_description,
                    'thoughts': thoughts,
                    'response': final_response
                })
                
                st.success("âœ… å·æ™®å·²å›æ‡‰ï¼")
                
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {str(e)}")
    
    # é¡¯ç¤ºå°è©±æ­·å²
    if hasattr(st.session_state, 'cot_history') and st.session_state.cot_history:
        st.divider()
        st.markdown("### ï¿½ å·æ™®çš„è©•è«–æ­·å²")
        
        for i, item in enumerate(st.session_state.cot_history[-5:], 1):  # åªé¡¯ç¤ºæœ€å¾Œ 5 æ¢
            with st.expander(f"ğŸ¤ è¨˜éŒ„ {i}: ã€Œ{item['event'][:40]}...ã€"):
                st.markdown(f"**è©±é¡Œ:** {item['event']}")
                st.markdown(f"**è©•è«–:** {item['thoughts']}")
                st.markdown(f"**å›æ‡‰:** {item['response']}")
