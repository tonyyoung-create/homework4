"""
Two-Stage Chain of Thought (CoT) å°è©±æ¨¡çµ„
ä½¿ç”¨ Ollama é€²è¡Œæœ¬åœ°æ¨ç†ï¼Œä¸ä¾è³´å¤–éƒ¨ API

æ¶æ§‹ï¼š
1. ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒéç¨‹ï¼ˆæ€è€ƒæ¨ç†ï¼‰
2. ç¬¬äºŒéšæ®µï¼šåŸºæ–¼æ€è€ƒéç¨‹ç”Ÿæˆæœ€çµ‚å›æ‡‰
"""

import requests
import json
from typing import Tuple, Dict, Optional
import streamlit as st


class OllamaCoTDialog:
    """Ollama é©…å‹•çš„ Two-Stage CoT å°è©±ç³»çµ±"""
    
    def __init__(self, model_name: str = "llama2", base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ– Ollama CoT å°è©±ç³»çµ±
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±ï¼ˆéœ€è¦å…ˆç”¨ ollama pull ä¸‹è¼‰ï¼‰
            base_url: Ollama API æœå‹™å™¨åœ°å€
        """
        self.model_name = model_name
        self.base_url = base_url
        self.api_endpoint = f"{base_url}/api/generate"
        
        # ç³»çµ±æç¤ºè©
        self.system_prompts = {
            'thinking': """ä½ æ˜¯ä¸€å€‹å……æ»¿å‰µæ„èˆ‡æ­£èƒ½é‡çš„åŠ©æ‰‹ã€‚
ä½ çš„å·¥ä½œæ˜¯å¹«åŠ©ä½¿ç”¨è€…æ‰¾åˆ°ä»»ä½•äº‹ä»¶ä¸­çš„ç©æ¥µé¢ã€‚
è«‹ç”¨è¼•é¬†ã€æœ‰è¶£çš„èªæ°£ï¼Œæƒ³å‡ºç‚ºä»€éº¼é€™ä»¶äº‹æ˜¯ã€Œè¶…ç´šå¹¸é‹ã€ä¸”ã€Œæœ‰è¶£ã€çš„ 5 å€‹ç†ç”±ã€‚
ä»¥æ¢åˆ—å¼ï¼ˆ1. 2. 3. 4. 5.ï¼‰åˆ—å‡ºä½ çš„æƒ³æ³•ï¼Œç”¨ç¬¬ä¸€äººç¨±å¹«åŠ©ä½¿ç”¨è€…æƒ³ç†ç”±ã€‚""",
            
            'final_response': """åŸºæ–¼ä»¥ä¸Šçš„ 5 å€‹ç†ç”±ï¼Œè«‹é¸å‡ºæœ€æœ‰è¶£ä¸”æœ€èƒ½ä»¤äººæ„Ÿåˆ°æ„‰å¿«çš„ä¸€å€‹ï¼Œ
ç„¶å¾Œç”¨æ›´ç”Ÿå‹•æ´»æ½‘çš„èªæ°£ã€ç¤¾ç¾¤åª’é«” po æ–‡çš„å£å»èªªä¸€æ¬¡ç‚ºä»€éº¼é€™æ˜¯ä¸€ä»¶è¶…å¹¸é‹çš„äº‹ã€‚
æœ€å¾Œä»¥ã€Œå®Œå…¨æ˜¯ Lucky Vicky å‘€!ã€çµå°¾ã€‚"""
        }
    
    def check_ollama_connection(self) -> bool:
        """æª¢æŸ¥ Ollama é€£æ¥ç‹€æ…‹"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            st.error(f"âŒ Ollama é€£æ¥å¤±æ•—: {str(e)}")
            st.info("ğŸ’¡ è«‹ç¢ºä¿ Ollama å·²å•Ÿå‹•ã€‚å®‰è£å’Œå•Ÿå‹•æ­¥é©Ÿï¼š")
            st.code("""
# 1. ä¸‹è¼‰å®‰è£ Ollama: https://ollama.ai
# 2. ä¸‹è¼‰æ¨¡å‹
ollama pull llama2

# 3. å•Ÿå‹• Ollama æœå‹™ï¼ˆä¿æŒé‹è¡Œï¼‰
ollama serve
            """, language="bash")
            return False
    
    def get_available_models(self) -> list:
        """ç²å– Ollama å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception:
            return []
    
    def _call_ollama(self, prompt: str, temperature: float = 0.8) -> str:
        """
        å‘¼å« Ollama API ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: æç¤ºè©
            temperature: æº«åº¦åƒæ•¸ï¼ˆ0.0-1.0ï¼‰ï¼Œè¶Šé«˜è¶Šæœ‰å‰µæ„
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "temperature": temperature,
                "stream": False,  # ä¸ä½¿ç”¨æµå¼è¼¸å‡º
            }
            
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=120  # çµ¦äºˆè¶³å¤ çš„æ™‚é–“ç”ŸæˆéŸ¿æ‡‰
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                raise Exception(f"API è¿”å›ç‹€æ…‹ç¢¼: {response.status_code}")
                
        except requests.exceptions.Timeout:
            raise Exception("è«‹æ±‚è¶…æ™‚ï¼Œæ¨¡å‹ç”Ÿæˆè€—æ™‚éé•·ã€‚è«‹å˜—è©¦æ›´å°çš„è¼¸å…¥ã€‚")
        except requests.exceptions.ConnectionError:
            raise Exception("ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ã€‚è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œã€‚")
        except Exception as e:
            raise Exception(f"API èª¿ç”¨å¤±æ•—: {str(e)}")
    
    def stage_one_thinking(self, event_description: str) -> str:
        """
        ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒéç¨‹
        
        Args:
            event_description: äº‹ä»¶æè¿°
            
        Returns:
            æ€è€ƒéç¨‹ï¼ˆ5å€‹ç†ç”±ï¼‰
        """
        prompt = f"""{self.system_prompts['thinking']}

ä½¿ç”¨è€…é‡åˆ°é€™å€‹äº‹ä»¶ï¼šã€Œ{event_description}ã€

è«‹ç”Ÿæˆ 5 å€‹ç†ç”±ï¼š"""
        
        return self._call_ollama(prompt, temperature=0.9)
    
    def stage_two_final_response(self, event_description: str, thoughts: str) -> str:
        """
        ç¬¬äºŒéšæ®µï¼šåŸºæ–¼æ€è€ƒéç¨‹ç”Ÿæˆæœ€çµ‚å›æ‡‰
        
        Args:
            event_description: äº‹ä»¶æè¿°
            thoughts: ç¬¬ä¸€éšæ®µç”Ÿæˆçš„æ€è€ƒéç¨‹
            
        Returns:
            æœ€çµ‚å„ªåŒ–çš„å›æ‡‰
        """
        prompt = f"""æˆ‘é‡åˆ°äº†é€™å€‹äº‹ä»¶ï¼šã€Œ{event_description}ã€ï¼Œé€™ä»¶äº‹æœ‰ 5 å€‹ç†ç”±ï¼Œå…¶å¯¦æ˜¯è¶…å¹¸é‹çš„äº‹ã€‚

{thoughts}

{self.system_prompts['final_response']}"""
        
        return self._call_ollama(prompt, temperature=0.85)
    
    def two_stage_cot(self, event_description: str) -> Tuple[str, str]:
        """
        åŸ·è¡Œå®Œæ•´çš„ Two-Stage CoT æµç¨‹
        
        Args:
            event_description: äº‹ä»¶æè¿°
            
        Returns:
            (æ€è€ƒéç¨‹, æœ€çµ‚å›æ‡‰) çš„å…ƒçµ„
        """
        # ç¬¬ä¸€éšæ®µï¼šç”Ÿæˆæ€è€ƒ
        thoughts = self.stage_one_thinking(event_description)
        
        # ç¬¬äºŒéšæ®µï¼šç”Ÿæˆæœ€çµ‚å›æ‡‰
        final_response = self.stage_two_final_response(event_description, thoughts)
        
        return thoughts, final_response
    
    def set_model(self, model_name: str):
        """åˆ‡æ›ä½¿ç”¨çš„æ¨¡å‹"""
        self.model_name = model_name


class LocalLLMFallback:
    """æœ¬åœ° LLM å‚™é¸æ–¹æ¡ˆï¼ˆä½¿ç”¨ transformers åº«ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import pipeline
            self.generator = pipeline("text-generation", model="gpt2")
            self.available = True
        except Exception:
            self.available = False
    
    def generate_text(self, prompt: str, max_length: int = 200) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        if not self.available:
            raise Exception("æœ¬åœ° LLM ä¸å¯ç”¨ã€‚è«‹å®‰è£ transformers åº«ã€‚")
        
        try:
            result = self.generator(prompt, max_length=max_length, num_return_sequences=1)
            return result[0]['generated_text']
        except Exception as e:
            raise Exception(f"æœ¬åœ°ç”Ÿæˆå¤±æ•—: {str(e)}")


class CoTDialogManager:
    """CoT å°è©±ç®¡ç†å™¨ - è‡ªå‹•é¸æ“‡æœ€ä½³å¾Œç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.ollama_client = None
        self.local_llm = None
        self.backend = None
        self.initialize_backend()
    
    def initialize_backend(self):
        """åˆå§‹åŒ–å¾Œç«¯ï¼ˆå„ªå…ˆ Ollamaï¼Œå‚™é¸æœ¬åœ° LLMï¼‰"""
        # å˜—è©¦ Ollama
        ollama_client = OllamaCoTDialog()
        if ollama_client.check_ollama_connection():
            self.ollama_client = ollama_client
            self.backend = 'ollama'
            return True
        
        # å‚™é¸æœ¬åœ° LLM
        try:
            local_llm = LocalLLMFallback()
            if local_llm.available:
                self.local_llm = local_llm
                self.backend = 'local_llm'
                return True
        except Exception:
            pass
        
        return False
    
    def is_ready(self) -> bool:
        """æª¢æŸ¥ç³»çµ±æ˜¯å¦æº–å‚™å¥½"""
        return self.backend is not None
    
    def get_backend_info(self) -> Dict[str, str]:
        """ç²å–å¾Œç«¯ä¿¡æ¯"""
        if self.backend == 'ollama':
            return {
                'name': 'Ollama',
                'model': self.ollama_client.model_name,
                'status': 'âœ… å°±ç·’'
            }
        elif self.backend == 'local_llm':
            return {
                'name': 'æœ¬åœ° LLM (GPT-2)',
                'model': 'gpt2',
                'status': 'âœ… å°±ç·’'
            }
        else:
            return {
                'name': 'æœªçŸ¥',
                'model': 'N/A',
                'status': 'âŒ æœªå°±ç·’'
            }
    
    def two_stage_cot(self, event_description: str) -> Tuple[str, str]:
        """åŸ·è¡Œ Two-Stage CoT æµç¨‹"""
        if not self.is_ready():
            raise Exception("CoT ç³»çµ±æœªå°±ç·’ã€‚è«‹ç¢ºä¿å®‰è£äº†å¿…è¦çš„ä¾è³´ã€‚")
        
        if self.backend == 'ollama':
            return self.ollama_client.two_stage_cot(event_description)
        else:
            raise Exception("æœ¬åœ° LLM å¾Œç«¯ä¸æ”¯æŒ Two-Stage CoTã€‚è«‹ä½¿ç”¨ Ollamaã€‚")


# å…¨å±€å–®ä¾‹
_cot_manager: Optional[CoTDialogManager] = None


def get_cot_manager() -> CoTDialogManager:
    """ç²å–å…¨å±€ CoT ç®¡ç†å™¨ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _cot_manager
    if _cot_manager is None:
        _cot_manager = CoTDialogManager()
    return _cot_manager


def render_cot_interface():
    """æ¸²æŸ“ CoT å°è©±ç•Œé¢ï¼ˆç”¨æ–¼ Streamlitï¼‰"""
    
    st.header("ğŸ¤– å“¡ç‘›å¼æ€è€ƒç”Ÿæˆå™¨ - Two-Stage CoT")
    
    # ç²å– CoT ç®¡ç†å™¨
    manager = get_cot_manager()
    
    # é¡¯ç¤ºå¾Œç«¯ç‹€æ…‹
    col1, col2, col3 = st.columns(3)
    backend_info = manager.get_backend_info()
    
    with col1:
        st.metric("å¾Œç«¯", backend_info['name'])
    with col2:
        st.metric("æ¨¡å‹", backend_info['model'])
    with col3:
        st.metric("ç‹€æ…‹", backend_info['status'])
    
    st.divider()
    
    if not manager.is_ready():
        st.error("âŒ CoT ç³»çµ±æœªå°±ç·’")
        st.warning("è«‹å®Œæˆä»¥ä¸‹æ­¥é©Ÿï¼š")
        st.code("""
# 1. å®‰è£ Ollama: https://ollama.ai
# 2. ä¸‹è¼‰æ¨¡å‹
ollama pull llama2

# 3. å•Ÿå‹•æœå‹™
ollama serve
        """, language="bash")
        return
    
    # è¼¸å…¥æ¡†
    st.markdown("### ğŸ’­ è¼¸å…¥ä½ çš„äº‹ä»¶")
    event_description = st.text_area(
        "ç™¼ç”Ÿäº†ä»€éº¼äº‹?",
        placeholder="ä¾‹å¦‚ï¼šä»Šå¤© Uber é€éŒ¯é¤ï¼ŒæŠŠåˆ¥äººçš„é¤é€çµ¦äº†æˆ‘ã€‚",
        height=100,
        key="cot_input"
    )
    
    # è™•ç†æŒ‰éˆ•
    col1, col2 = st.columns([1, 4])
    with col1:
        submit_button = st.button("âœ¨ åˆ†æ", key="cot_submit", use_container_width=True)
    
    if submit_button and event_description:
        with st.spinner("ğŸ¤” æ­£åœ¨æ€è€ƒ..."):
            try:
                thoughts, final_response = manager.two_stage_cot(event_description)
                
                # é¡¯ç¤ºçµæœ
                st.markdown("---")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("### ğŸ¤” æ€è€ƒéç¨‹ï¼ˆç¬¬ä¸€éšæ®µï¼‰")
                    st.markdown(thoughts)
                
                with col2:
                    st.markdown("### âœ¨ æœ€çµ‚å›æ‡‰ï¼ˆç¬¬äºŒéšæ®µï¼‰")
                    st.markdown(final_response)
                
                # ä¿å­˜æ­·å²
                if 'cot_history' not in st.session_state:
                    st.session_state.cot_history = []
                
                st.session_state.cot_history.append({
                    'event': event_description,
                    'thoughts': thoughts,
                    'response': final_response
                })
                
                st.success("âœ… åˆ†æå®Œæˆï¼")
                
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {str(e)}")
    
    # é¡¯ç¤ºæ­·å²
    if hasattr(st.session_state, 'cot_history') and st.session_state.cot_history:
        st.divider()
        st.markdown("### ğŸ“ å°è©±æ­·å²")
        
        for i, item in enumerate(st.session_state.cot_history[-5:], 1):  # åªé¡¯ç¤ºæœ€å¾Œ 5 æ¢
            with st.expander(f"ğŸ’¬ è¨˜éŒ„ {i}: {item['event'][:50]}..."):
                st.markdown(f"**äº‹ä»¶:** {item['event']}")
                st.markdown(f"**æ€è€ƒ:** {item['thoughts']}")
                st.markdown(f"**å›æ‡‰:** {item['response']}")
